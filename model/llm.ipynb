{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shravya H Jain\\Desktop\\wiseverse\\llm\\llm_\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\Shravya H Jain\\Desktop\\wiseverse\\llm\\llm_\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shravya H Jain\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9992625117301941}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier(\"I love using Hugging Face models!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shravya H Jain\\Desktop\\wiseverse\\llm\\llm_\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shravya H Jain\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The future of AI is being explored by a new group of developers at Google and Intel\\n\\nA lot of AI technologies have been exploring on AI, not because of great success, but to see what their potential impact will be in their daily lives.\\n\\nOne such discovery is that \"AI is currently trying to learn from real-world experiences that people\\'ve had. For some people that means learning to solve problems they have with different kinds of technology in the past.\"\\n\\nThis idea, described'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "text = generator(\"The future of AI is\", max_length=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")  \n",
    "outputs = model(**inputs)  #input_ids are the numerical representations of the tokens.attention_mask is used to tell the model which tokens are actual words and which are padding\n",
    "predictions = outputs.logits.argmax(dim=-1) #argmax find the maximum value in the logits produced and dim=-1 means its from the last class/layer\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Greetings, Young Minds!\n",
      "\n",
      "We are thrilled to announce SkillHigh's Internship Programâ€”a golden opportunity to elevate your career in both IT and non-IT domains. Crafted in collaboration with industry leaders like Wipro, Microsoft, EY, IBM, and Adobe, this program offers hands-on experience, practical insights, and industry-recognized certifications to prepare you for the professional world.\n",
      "Key Program Features:\n",
      "\n",
      "Assured Stipend starting from â‚¹15,000\n",
      "Engage in Application-Based Projects\n",
      "Access to Premium Study Materials\n",
      "100% Internship Assurance\n",
      "Wipro Certifications to boost your profile\n",
      "88% Placement Support for future opportunities\n",
      "Domains.jpeg\n",
      "Explore Exciting Domains:\n",
      "Full Stack Web Development \n",
      "Cyber Security\n",
      "Machine Learning & AI\n",
      "Data Science\n",
      "Embedded Systems \n",
      "Hybrid Electric Vehicle\n",
      "AutoCAD & BIM\n",
      "IoT\n",
      "Digital Marketing \n",
      "Business Analytics\n",
      "UI/UX\n",
      "VSLI\n",
      "DSA\n",
      "Finance\n",
      "App development\n",
      "HR\n",
      "Limited Seats! Apply now to secure your spot:\n",
      "ðŸ”— Application Form:https://forms.gle/Xii6YdNmsJM5FJJK8\n",
      "Referral Code: WIPRO24\n",
      "Act Now and take the first step towards a rewarding career! ðŸš€\n",
      "ðŸ“Œ Note: Enrollment is first-come, first-served. Donâ€™t miss out!\n",
      "Regards,\n",
      "Sai Lakshmi | Lead Gen | SkillHigh\n",
      "WEBSITE/LINKEDIN\n",
      "Prediction: Spam\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained spam detection model and tokenizer\n",
    "model_name = \"mrm8488/bert-tiny-finetuned-sms-spam-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"Greetings, Young Minds!\n",
    "\n",
    "We are thrilled to announce SkillHigh's Internship Programâ€”a golden opportunity to elevate your career in both IT and non-IT domains. Crafted in collaboration with industry leaders like Wipro, Microsoft, EY, IBM, and Adobe, this program offers hands-on experience, practical insights, and industry-recognized certifications to prepare you for the professional world.\n",
    "Key Program Features:\n",
    "\n",
    "Assured Stipend starting from â‚¹15,000\n",
    "Engage in Application-Based Projects\n",
    "Access to Premium Study Materials\n",
    "100% Internship Assurance\n",
    "Wipro Certifications to boost your profile\n",
    "88% Placement Support for future opportunities\n",
    "Domains.jpeg\n",
    "Explore Exciting Domains:\n",
    "Full Stack Web Development \n",
    "Cyber Security\n",
    "Machine Learning & AI\n",
    "Data Science\n",
    "Embedded Systems \n",
    "Hybrid Electric Vehicle\n",
    "AutoCAD & BIM\n",
    "IoT\n",
    "Digital Marketing \n",
    "Business Analytics\n",
    "UI/UX\n",
    "VSLI\n",
    "DSA\n",
    "Finance\n",
    "App development\n",
    "HR\n",
    "Limited Seats! Apply now to secure your spot:\n",
    "ðŸ”— Application Form:https://forms.gle/Xii6YdNmsJM5FJJK8\n",
    "Referral Code: WIPRO24\n",
    "Act Now and take the first step towards a rewarding career! ðŸš€\n",
    "ðŸ“Œ Note: Enrollment is first-come, first-served. Donâ€™t miss out!\n",
    "Regards,\n",
    "Sai Lakshmi | Lead Gen | SkillHigh\n",
    "WEBSITE/LINKEDIN\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Map prediction to label\n",
    "labels = [\"Not Spam\", \"Spam\"]\n",
    "predicted_label = labels[predictions.item()]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Prediction: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The team won the championship in table tennis.\n",
      "Predicted Topic: Sports\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained topic classification model and tokenizer\n",
    "model_name = \"textattack/distilbert-base-uncased-ag-news\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)  \n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Input text\n",
    "text = \"The team won the championship in table tennis.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Map prediction to label\n",
    "labels = [\"Sports\", \"Politics\", \"Technology\",\"commercial\"]\n",
    "predicted_label = labels[predictions.item()]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Predicted Topic: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"your_dataset\")  # Replace with your dataset\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"textattack/distilbert-base-uncased-ag-news\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"your_dataset\")  # Replace with your dataset\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    \"sports\": 0,\n",
    "    \"politics\": 1,\n",
    "    \"technology\": 2,\n",
    "    \"entertainment\": 3\n",
    "}\n",
    "\n",
    "# Preprocess dataset to map labels\n",
    "def preprocess_labels(examples):\n",
    "    examples[\"label\"] = [label_mapping[label] for label in examples[\"label\"]]\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.map(preprocess_labels)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"textattack/distilbert-base-uncased-ag-news\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",  # Save model at each epoch\n",
    "    load_best_model_at_end=True,  # Load best model based on evaluation metric\n",
    "    metric_for_best_model=\"accuracy\",  # Metric to use for best model selection\n",
    "    greater_is_better=True,  # Whether the metric is better when greater\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=lambda pred: {\"accuracy\": torch.sum(pred.label_ids == pred.predictions.argmax(-1)) / len(pred.label_ids)},\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
